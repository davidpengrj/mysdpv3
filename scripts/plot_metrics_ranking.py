import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import spearmanr
import os

PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
DATA_DIR     = os.path.join(PROJECT_ROOT, 'data')
FIG_DIR      = os.path.join(PROJECT_ROOT, 'figures')

# === 1. ç»˜å›¾é£æ ¼è®¾ç½® ===
# ä¿æŒä½¿ç”¨ 'poster' é£æ ¼ï¼Œè¿™æ ·å­—ä½“å’Œçº¿æ¡é»˜è®¤éƒ½ä¼šæ¯”è¾ƒç²—å¤§ï¼Œé€‚åˆè®ºæ–‡/PPT
sns.set(style="whitegrid", context="poster") 

plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial', 'SimHei']  # é€‚é…ä¸­æ–‡
plt.rcParams['axes.unicode_minus'] = False 
plt.rcParams['pdf.fonttype'] = 42 # ç¡®ä¿PDFæ–‡æœ¬å¯ç¼–è¾‘

def plot_metric_effectiveness_ranking(file_path, target_perf_metric):
    """
    ç”ŸæˆæŒ‡æ ‡æœ‰æ•ˆæ€§æ’åç®±çº¿å›¾ (å»é™¤äº†å·¦ä¸‹è§’/å³ä¸Šè§’çš„æ–‡å­—æ³¨é‡Š)
    """
    print(f"\n>>> æ­£åœ¨å¤„ç†æ€§èƒ½æŒ‡æ ‡: {target_perf_metric} ...")

    # --- æ•°æ®åŠ è½½ ---
    if not os.path.exists(file_path):
        print(f"âŒ æ‰¾ä¸åˆ°æ–‡ä»¶: {file_path}")
        return

    try:
        df = pd.read_csv(file_path)
    except Exception as e:
        print(f"âŒ è¯»å–æ•°æ®å¤±è´¥: {e}")
        return

    # --- å®šä¹‰ UQ æŒ‡æ ‡ ---
    uq_metrics_candidates = [
        'Entropy', 'Confidence', 'LeastConf', 'Margin', 
        'DeepGini', 'Variance', 'ExpEntropy', 'BALD'
    ]
    uq_metrics = [m for m in uq_metrics_candidates if m in df.columns]
    
    if target_perf_metric not in df.columns:
        print(f"âš ï¸ è·³è¿‡: CSV ä¸­æœªæ‰¾åˆ°åˆ— '{target_perf_metric}'")
        return

    # --- è®¡ç®— Spearman ç›¸å…³æ€§ ---
    correlation_data = []
    models = df['Model'].unique()
    
    for uq in uq_metrics:
        for model in models:
            sub_df = df[df['Model'] == model][[uq, target_perf_metric]].dropna()
            if len(sub_df) > 5:
                corr, _ = spearmanr(sub_df[uq], sub_df[target_perf_metric])
                correlation_data.append({
                    'UQ Metric': uq,
                    'Correlation': corr,
                    'Model': model
                })
    
    res_df = pd.DataFrame(correlation_data)
    if res_df.empty:
        print(f"âš ï¸ æ— æ³•è®¡ç®— {target_perf_metric} çš„ç›¸å…³æ€§ã€‚")
        return

    # --- ç»˜å›¾é€»è¾‘ ---
    # ç”»å¸ƒä¿æŒè¾ƒå¤§å°ºå¯¸ (16x10)ï¼Œä¿è¯æ¸…æ™°åº¦
    plt.figure(figsize=(16, 10))
    
    # æ’åºï¼šæŒ‰ä¸­ä½æ•°ä»å°åˆ°å¤§æ’åº
    order = res_df.groupby('UQ Metric')['Correlation'].median().sort_values().index
    
    # ç»˜åˆ¶ç®±çº¿å›¾ - çº¿å®½è®¾ä¸º 2.5ï¼Œéå¸¸æ¸…æ™°
    sns.boxplot(x='UQ Metric', y='Correlation', data=res_df, order=order, 
                palette="RdBu_r", showfliers=False, width=0.6, linewidth=2.5)
    
    # æ·»åŠ æ•£ç‚¹ - é¢œè‰²åŠ æ·±ï¼Œå¤§å°é€‚ä¸­
    sns.stripplot(x='UQ Metric', y='Correlation', data=res_df, order=order,
                  color='#333333', alpha=0.6, jitter=True, size=8)
    
    # 0åˆ»åº¦å‚è€ƒçº¿
    plt.axhline(0, color='gray', linestyle='--', linewidth=2, alpha=0.8)
    
    # --- æ ‡é¢˜å’Œæ ‡ç­¾ (å·²ç§»é™¤ plt.text æ³¨é‡Š) ---
    plt.title(f'Metric Effectiveness: Spearman Correlation with {target_perf_metric}', 
              fontsize=24, fontweight='bold', pad=25)
    
    plt.ylabel(f'Spearman Correlation (vs {target_perf_metric})', fontsize=20, labelpad=15)
    plt.xlabel('Uncertainty Quantification Metrics', fontsize=20, labelpad=15)
    
    # åæ ‡è½´åˆ»åº¦å­—ä½“
    plt.xticks(rotation=30, fontsize=16)
    plt.yticks(fontsize=16)

    # --- ä¿å­˜ ---
    safe_name = target_perf_metric.replace('(', '').replace(')', '').replace('/', '_')
    output_file = os.path.join(FIG_DIR, f"Experiment1_Ranking_{safe_name}.pdf")
    
    plt.savefig(output_file, bbox_inches='tight', dpi=300)
    print(f"âœ… å›¾è¡¨å·²ä¿å­˜: {output_file}")
    plt.close()

if __name__ == "__main__":
    file_name = os.path.join(DATA_DIR, "benchmark_results_IVDP_FullUQ.csv")
    
    target_metrics = ['MCC', 'F1', 'AUC', 'Recall(PD)', 'Precision', 'FPR(PF)']
    
    print(f"ğŸš€ å¼€å§‹æ‰¹é‡ç”Ÿæˆ 6 å¼ å›¾è¡¨...")
    for metric in target_metrics:
        plot_metric_effectiveness_ranking(file_name, target_perf_metric=metric)
    print("\nğŸ‰ æ‰€æœ‰å›¾è¡¨ç”Ÿæˆå®Œæ¯•ï¼")